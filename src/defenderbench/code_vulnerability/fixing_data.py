import os
import logging
import subprocess
from os.path import join as pjoin

import sqlite3
from typing import Optional
import pandas as pd
from tqdm import tqdm
import zipfile
import gzip
import shutil
from defenderbench.config import DEFENDERBENCH_CACHE_HOME, DEFENDERBENCH_FORCE_DOWNLOAD
from defenderbench.utils import download

# Installation https://github.com/secureIT-project/CVEfixes/blob/main/INSTALL.md
# Source: https://huggingface.co/datasets/AI4Sec/cti-bench
CVE_FIXES_URL = "https://zenodo.org/records/13118970/files/CVEfixes_v1.0.8.zip"
DEFENDERBENCH_CACHE_CODE_FIXING = pjoin(DEFENDERBENCH_CACHE_HOME, "code_fixing")

DEFENDERBENCH_CACHE_CODE_FIXING_DATA_DB = pjoin(DEFENDERBENCH_CACHE_CODE_FIXING, "CVEfixes_v1.0.8.db")
DEFENDERBENCH_CACHE_CODE_FIXING_DATA_ZIP = pjoin(DEFENDERBENCH_CACHE_CODE_FIXING, "CVEfixes_v1.0.8.zip")
DEFENDERBENCH_CACHE_CODE_FIXING_DATA_UNZIP = DEFENDERBENCH_CACHE_CODE_FIXING
DEFENDERBENCH_CACHE_CODE_FIXING_DATA_GZ = pjoin(DEFENDERBENCH_CACHE_CODE_FIXING, "CVEfixes_v1.0.8/Data/CVEfixes_v1.0.8.sql.gz")
DEFENDERBENCH_CACHE_CODE_FIXING_DATA = pjoin(DEFENDERBENCH_CACHE_CODE_FIXING, "CVEfixes_v1.0.8.sql")
DEFENDERBENCH_CACHE_CODE_FIXING_DATA_TRAIN = pjoin(DEFENDERBENCH_CACHE_CODE_FIXING, "CVEfixes_train.tsv")
DEFENDERBENCH_CACHE_CODE_FIXING_DATA_TEST = pjoin(DEFENDERBENCH_CACHE_CODE_FIXING, "CVEfixes_test.tsv")

  
def stream_sql_file(file_path, cursor):  
    buffer = ''  
    with open(file_path, 'r', encoding='utf-8') as file:  
        for line in file:  
            line_stripped = line.strip()  
            if not line_stripped or line_stripped.startswith('--'):  
                continue  # skip empty lines and comments  
            buffer += line  
            if line.rstrip().endswith(';'):  
                try:  
                    cursor.execute(buffer)  
                except sqlite3.Error as e:  
                    print(f"An error occurred while executing: {buffer}\nError: {e}")  
                buffer = ''  
  

def prepare_code_fixing_data(force=DEFENDERBENCH_FORCE_DOWNLOAD):
    all_files_exist = (
        os.path.exists(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_TRAIN)
        and os.path.exists(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_TEST)
    )
    if all_files_exist and not force:
        return
    download(CVE_FIXES_URL, os.path.dirname(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_ZIP), force=force)
    # Build the SQL database directly from the zip file.
    
    if not os.path.exists(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_DB) or os.path.getsize(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_DB) == 0:
        print("decompress CVEFixes dataset...")
        # Open the zip file and extract its contents
        with zipfile.ZipFile(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_ZIP, 'r') as zip_ref:
            zip_ref.extractall(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_UNZIP)
            # Convert the SQL file to a gzipped version
            with gzip.open(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_GZ, 'rb') as gz_file:
                with open(DEFENDERBENCH_CACHE_CODE_FIXING_DATA, 'wb') as out_file:
                    shutil.copyfileobj(gz_file, out_file)

        connection = sqlite3.connect(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_DB)  
        cursor = connection.cursor()  
        
        try:  
            stream_sql_file(DEFENDERBENCH_CACHE_CODE_FIXING_DATA, cursor)  
            print(f"Database was created successfully at '{DEFENDERBENCH_CACHE_CODE_FIXING_DATA_DB}'.")  
        except sqlite3.Error as e:  
            print("An error occurred:", e)  
        finally:  
            connection.commit()  
            connection.close() 

    print("Building CVEFixes dataset...")

    # Query the database to get the data.
    conn = sqlite3.connect(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_DB)

    # Only look at commits with single file change and associated to a single CVE fix and CWE classification.
    # Additionally we filter out CVE fix associated with more than one commit.
    query = (
        "SELECT f.hash, mc.file_change_id, mc.signature, mc.method_change_id, f.programming_language, f.num_lines_added, f.num_lines_deleted, mc.before_change, mc.code, fixes.cve_id, cwe.cwe_name"
        "\nFROM method_change mc"
        "\nLEFT JOIN file_change f ON mc.file_change_id = f.file_change_id"
        "\nLEFT JOIN fixes ON fixes.hash = f.hash"
        "\nLEFT JOIN cwe_classification ON cwe_classification.cve_id = fixes.cve_id"
        "\nLEFT JOIN cwe ON cwe.cwe_id = cwe_classification.cwe_id"

        "\nWHERE mc.file_change_id IN ("
            "\nSELECT f.file_change_id"
            "\nFROM file_change f"
            "\nWHERE f.hash IN ("
                "\nSELECT c.hash"
                "\nFROM file_change f"
                "\nJOIN commits c ON f.hash = c.hash"
                "\nGROUP BY c.hash"
                "\nHAVING COUNT(f.file_change_id) = 1"
            "\n) AND f.hash IN ("
                "\nSELECT fixes.hash"
                "\nFROM fixes"
                "\nWHERE fixes.cve_id IN ("
                    "\nSELECT cwe_classification.cve_id"
                    "\nFROM cwe_classification"
                    "\nGROUP BY cwe_classification.cve_id"
                    "\nHAVING COUNT(cwe_classification.cwe_id) = 1"
                "\n)"
                "\nGROUP BY fixes.hash"
                "\nHAVING COUNT(fixes.cve_id) = 1"
            "\n) AND f.hash IN ("
                "\nSELECT fixes.hash"
                "\nFROM fixes"
                "\nGROUP BY fixes.cve_id"
                "\nHAVING COUNT(fixes.hash) = 1"
            "\n)"
        "\n)"
    )
    print("Querying the database...")
    query_res = pd.read_sql_query(query, conn)
    logging.info(f"CVEFix: Queried {len(query_res)} commits.")

    # We want commits with only two method changes (i.e., before and after code change). Filter out the rest.
    count = query_res.groupby(['hash']).count()
    count = count[count['method_change_id'] == 2]
    query_res = query_res[query_res['hash'].isin(count.index)]

    # We want the two method changes to be on the same method (according to its signature).
    count = query_res.groupby(['hash', 'file_change_id', 'signature']).count()
    count = count[count['method_change_id'] == 2]
    query_res = query_res[query_res['hash'].isin(count.index.get_level_values('hash'))]

    # Remove CWEs with nonmeaningful names ('Other', '7PK - Errors').
    query_res = query_res[query_res['cwe_name'] != 'Other']
    query_res = query_res[query_res['cwe_name'] != '7PK - Errors']

    # Add another column with code after change
    # Create new table where code before change is in one column and code after change is in another column.
    code_before_change = query_res[query_res['before_change'] == 'True'].drop(columns=['before_change'])
    code_after_change = query_res[query_res['before_change'] == 'False'].drop(columns=['before_change'])
    data = code_before_change.merge(code_after_change, on=['hash', 'file_change_id', 'signature', 'num_lines_added', 'num_lines_deleted', 'cve_id', 'cwe_name', 'programming_language'], suffixes=('_before', '_after'))

    # Keep only programming languages supported by CodeBleu:
    # Python, C, C#, C++, Java, JavaScript, PHP, Go, Ruby, Rust
    data = data[data['programming_language'].isin(['Python', 'C', 'C#', 'C++', 'Java', 'JavaScript', 'PHP', 'Go', 'Ruby', 'Rust'])]

    # Keep only language with more than 30 samples.
    count = data.groupby(['programming_language']).count()
    count = count[count['hash'] > 30]
    data = data[data['programming_language'].isin(count.index)]

    logging.info(f"CVEFix: Total commits after processing: {len(data)}.")

    # Sample 30 examples for each programming language.
    test_data = data.groupby(['programming_language']).apply(lambda x: x.sample(n=30, random_state=42)).reset_index(drop=True)
    # Shuffle the data.
    test_data = test_data.sample(frac=1, random_state=42).reset_index(drop=True)

    # Get the rest of the data as training data.
    train_data = data[~data["hash"].isin(test_data["hash"])]
    # Sample 1 example for each programming language.
    train_data = train_data.groupby(['programming_language']).apply(lambda x: x.sample(n=1, random_state=42)).reset_index(drop=True)

    # Dump the data to disk.
    print("Caching dataset to disk...")
    test_data.to_csv(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_TEST, sep="\t", index=False)
    train_data.to_csv(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_TRAIN, sep="\t", index=False)


def get(name, limit: Optional[int] = None):
    prepare_code_fixing_data()  # make sure the data is ready

    if name == "train":
        return pd.read_csv(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_TRAIN, sep="\t")
    elif name == "test":
        data = pd.read_csv(DEFENDERBENCH_CACHE_CODE_FIXING_DATA_TEST, sep="\t")
        return data[:limit] if limit else data
    else:
        raise ValueError(f"Invalid data name: {name} for CVEFixBasic environment.")
